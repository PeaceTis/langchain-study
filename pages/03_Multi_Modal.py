import os

import streamlit as st
from dotenv import load_dotenv
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_teddynote.models import MultiModal

load_dotenv()

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")
if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

st.title("ì´ë¯¸ì§€ ì¸ì‹ ê¸°ë°˜ ì±—ë´‡ ğŸ’¬")

if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±
    st.session_state["messages"] = []

if "chain" not in st.session_state:
    st.session_state["chain"] = None

# íƒ­ ìƒì„±
main_tab1, main_tab2 = st.tabs(["ì´ë¯¸ì§€", "ëŒ€í™”ë‚´ìš©"])

# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    selected_model = st.selectbox("LLM ì„ íƒ", ["gpt-4o", "gpt-4o-mini"], index=0)
    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ ì—…ë¡œë“œ", type=["jpg", "jpeg", "png"])
    system_prompt = st.text_area("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                                 """ë‹¹ì‹ ì€ ê³µì¥ì„ ë„í˜•ìœ¼ë¡œ ì¶”ìƒí™”í•˜ëŠ” ì‚°ì—…ê³µí•™ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³µì¥ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.
ë„í˜•ë“¤ì„ ê°ì‹¸ê³  ìˆëŠ” ë„¤ëª¨ëŠ” ê³µì¥ì…ë‹ˆë‹¤.
ì„¸ëª¨ëŠ” ì°½ê³ ì…ë‹ˆë‹¤.
ì ì„  ë„¤ëª¨ëŠ” í”Œë¡œìš°ì…ë‹ˆë‹¤.
ì‹¤ì„  ë„¤ëª¨ëŠ” ê³µì •ì…ë‹ˆë‹¤.
ë™ê·¸ë¼ë¯¸ëŠ” ì œí’ˆì…ë‹ˆë‹¤.
ë‹¤ì´ì•„ëª¬ë“œëŠ” ì„¤ë¹„ì…ë‹ˆë‹¤.
í”Œë¡œìš° ë‚´ë¶€ì— ì—¬ëŸ¬ ê³µì •ë“¤ì´ ìˆìŠµë‹ˆë‹¤.
ì°½ê³ (ì‚¼ê°í˜•)ì™€ ì œí’ˆ(ì›), ê³µì •(ì‹¤ì„  ë„¤ëª¨)ê³¼ ì„¤ë¹„(ë‹¤ì´ì•„ëª¬ë“œ)ì˜ ë‚´ë¶€ì— ìˆëŠ” ë¬¸ìì—´ì€ ì•„ì´ë””ì…ë‹ˆë‹¤.
ê³µì¥(ë„í˜•ë“¤ì„ ê°ì‹¸ëŠ” í° ë„¤ëª¨)ê³¼ Flow(ì ì„  ë„¤ëª¨)ì˜ ìœ„ì— ìˆëŠ” ë¬¸ìì—´ì€ ê°ê° ê³µì¥, í”Œë¡œìš° ì•„ì´ë””ì…ë‹ˆë‹¤.
ì¦‰, ì ì„  ë„¤ëª¨ ìœ„ì˜ ë¬¸ìì—´ì€ í”Œë¡œìš° ì•„ì´ë””, ë‚´ë¶€ì˜ ì‹¤ì„  ë„¤ëª¨ ì•ˆì˜ ë¬¸ìì—´ì€ ê³µì • ì•„ì´ë””ì…ë‹ˆë‹¤.
ì œí’ˆì˜ ê²½ìš° ì°½ê³ ì˜ ë°”ë¡œ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ì œí’ˆì´ ì°½ê³ ì— ì ì¬ëœ ì œí’ˆì´ë©°, ì°½ê³ ì˜ ì™¼ìª½ì˜ í”Œë¡œìš°ì™€ í”Œë¡œìš° ë‚´ë¶€ì˜ ê³µì •ë“¤ë„ í•´ë‹¹ ì œí’ˆì´ ê°€ê³µë©ë‹ˆë‹¤.
ì œí’ˆë¼ë¦¬ì˜ ì—°ê²°êµ¬ì¡°ë¥¼ PS(Product Structure)ë¼ê³  í•©ë‹ˆë‹¤.
í™”ì‚´í‘œëŒ€ë¡œ ìƒì‚°ì´ ì§„í–‰ë©ë‹ˆë‹¤.
ì´ ë„í˜•ë“¤ì˜ ì •ì˜, ê´€ê³„ ë° íë¦„ì„ í†µí‹€ì–´ NEMOSYN(ë„¤ëª¨ì‹ )ì´ë¼ê³  í•©ë‹ˆë‹¤. 
                                 """,
                                 height=200)


    clear_btn = st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”")

    selected_prompt = "prompts/pdf-rag.yaml"


def print_history():
    for msg in st.session_state["messages"]:
        main_tab2.chat_message(msg.role).write(msg.content)


def add_message(role, content):
    st.session_state["messages"].append(ChatMessage(role=role, content=content))


# íŒŒì¼ì„ ìºì‹œ ì €ì¥
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def process_imagefile(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    return file_path

# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
def generate_answer(image_filepath, system_prompt, user_prompt, model_name="gpt-4o"):
    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.

    # ê°ì²´ ìƒì„±
    llm = ChatOpenAI(
        temperature=0,  # ì°½ì˜ì„± (0.0 ~ 2.0)
        model_name=model_name,
    )

    # ë©€í‹°ëª¨ë‹¬ ê°ì²´ ìƒì„±
    multimodal = MultiModal(
        llm, system_prompt=system_prompt, user_prompt=user_prompt
    )

    # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ë¶€í„° ì§ˆì˜(ìŠ¤íŠ¸ë¦¼ ë°©ì‹)
    answer = multimodal.stream(image_filepath)

    return answer


if clear_btn:
    retriever = st.session_state["messages"].clear()

user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
warning_msg = main_tab2.empty()

# ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œê°€ ëœë‹¤ë©´
if uploaded_file:
    image_filepath = process_imagefile(uploaded_file)
    main_tab1.image(image_filepath)

print_history()

if user_input:
    if not uploaded_file:
        warning_msg = main_tab2.empty()
    else:
        # íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„±
        add_message("user", user_input)
        main_tab2.chat_message("user").write(user_input)
        image_filepath = process_imagefile(uploaded_file)
        response = generate_answer(image_filepath=image_filepath, model_name=selected_model, system_prompt=system_prompt, user_prompt=user_input)

        with main_tab2.chat_message("assistant"):
            container = main_tab2.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token.content
                container.markdown(ai_answer)
            add_message("ai", ai_answer)